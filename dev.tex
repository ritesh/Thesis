\chapter{Design \& Implementation}
\label{ch:dev}
In this chapter the design and implementation of the experimental setup are discussed. The web crawler and the analysis module generate results that are stored for evaluation later.
\section {Web Scraper}
\label{sec:scraper}
%What is a scraper?
%What does it consist of?
%Why do you need it for these project?
%Why did you choose scrapy?
%How does it work?
A scraper allows an application to retrieve and store specific items from a web page. Any part of the HTML Document Object Model (DOM) can be selected and extracted for further processing, a process known as scraping. Most crawlers consist of two main components, a web crawler that allows it to collect a large number of pages quickly and the actual scraping module that allows for the extraction of relevant content. A large number of free and open source projects are available that provide both kinds of functionality. For this project, scrapy was used as the basis for the web scraper based on previous evaluations and it's excellent database connectivity and image handling options.
\par Scrapy is a python web framework that can be used to build web crawlers/scrapers. It was designed primarily for programmatically accessing websites that do not explicitly provide an Application Programming Interface (API). Scrapy does not provide an implementation of a specific scraper, it provides a number of classes that can be used to create a custom scraper. A scrapy project consists of one or more spiders and pipelines that determine what happens once an item has been downloaded. The crawler component was created by extending scrapy's \texttt{BaseSpider} class. The \texttt{BaseSpider} class requires the implementation of a \texttt{parse()} method that determines the action to be taken when an HTML page is encountered. The \texttt{parse()} method is called on every invocation of the spider. Relevant elements in the HTML page are selected using HTML XPath queries. In this implementation, the \texttt{parse()} method yields subsequent HTTP requests that the crawler needs to make subsequently using the URLs found on the page. An \texttt{Item} class is also created for each JPEG image found on the page. These images are then added to a python dictionary and returned to the \texttt{ImagePipeline} middleware.
\par The \texttt{ImagePipeline} middleware is responsible for the scheduling and downloading of image files. For each dictionary of items returned by the crawler, the pipeline enqueues the URLs of the images found and downloads them asynchronously. Saved images are named according to the MD5 hash of their content so that they can be uniquely identified at a later stage. After building a suitably large collection of images, steganalysis was carried out on each of the images as described in the next section.
\section {Steganalysis}
\label{sec:stegtool}
Steganalysis on the downloaded images is carried out using \texttt{stegdetect} which is part of Provos' Outguess suite of steganography tools. \texttt{stegdetect} provides detection features for six different kinds of JPEG steganographic algorithms namely jsteg, outguess, jphide, invisible secrets, F5 and text appending. For each steganographic algorithm detected by \texttt{stegdetect} it provides a confidence rating between one to three stars. It also provides a parameter for tuning the sensitivity of the scanner. The sensitivity can be specified as a floating point number, which would cause the confidence rating to be multiplied by it. For this project, \texttt{stegdetect} was used only to try and detect algorithms known to it although stegdetect allows for feature extraction that can be used to detect unknown algorithms. The detection was carried out using four different sensitivity settings (0.25,0.5,0.75 and 1) in order to minimize false positives.
\par A custom python script was created to wrap \texttt{stegdetect} and generate output in a way suitable for database storage. Finally, the data obtained was saved in a database for further analysis.
\section{Database Structure}
\label{sec:dbstructure}
All data generated from the Web Scraper as well as from the analysis stage are stored in a MySQL database. A single data table stores details of the downloaded images which are generated by the middleware after each image has been downloaded. Each image is uniquely identified in the database by an auto incrementing primary key. Steganalysis is carried out by obtaining the path of each image from the database and running stegdetect on them. The results of the stegnalayis are stored in an analysis table which uniquely identifies each result by an autoincrementing primary key and references the primary key of the image from the data table. The initial design used an sqlite embedded database but due to frequent thread errors caused by concurrent access, the database was migrated to MySQL. All access to the database is through a connection pool that maintains 4 connections to the MySQL database. This was implemented using a class from the python twisted library called the Asynchronous Database Access Application Programming Interface (adbapi). Adbapi provides a non blocking interface to the database and vastly improves multiple read/write access when compared to single connection database APIs (e.g. the standard python MySQLdb API). 

